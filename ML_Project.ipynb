{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTOhwNXAL6no"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Problem Statement\n",
        "Problem Statement\n",
        " Cryptocurrency markets are highly volatile, and understanding and forecasting this volatility is crucial for\n",
        "market participants. Volatility refers to the degree of variation in the price of a cryptocurrency over time, and\n",
        "high volatility can lead to significant risks for traders and investors. Accurate volatility prediction helps in risk\n",
        "management, portfolio allocation, and developing trading strategies.\n",
        " In this project, you are required to build a machine learning model to predict cryptocurrency volatility levels\n",
        "based on historical market data such as OHLC (Open, High, Low, Close) prices, trading volume, and market\n",
        "capitalization. The objective is to anticipate periods of heightened volatility, enabling traders and financial\n",
        "institutions to manage risks and make informed decisions.\n",
        " Your final model should provide insights into market stability by forecasting volatility variations, allowing\n",
        "stakeholders to proactively respond to changing market conditions.\n",
        " Dataset Information\n",
        " You will use a dataset that includes historical daily cryptocurrency price, volume, and market capitalization\n",
        "data for multiple cryptocurrencies.\n",
        " Dataset:\n",
        " Cryptocurrency Historical Prices Dataset\n",
        " Data Preprocessing Require\n",
        " Handle missing values and ensure data consistenc\n",
        " Normalize and scale numerical feature\n",
        " Engineer new features related to volatility and liquidity trends\n",
        " The dataset consists of daily records for over 50 cryptocurrencies, including features such as date, symbol,\n",
        "open, high, low, close, volume, and market cap.\n",
        " Project Development Step\n",
        " Data Collection: Gather historical OHLC, volume, and market cap data from the provided datase\n",
        " Data Preprocessing: Handle missing values, clean data, and normalize numerical feature\n",
        " Exploratory Data Analysis (EDA): Analyze data patterns, trends, and correlation\n",
        " Feature Engineering: Create relevant features such as moving averages, rolling volatility, liquidity ratios (e.g.,\n",
        "volume/market cap), and technical indicators (e.g., Bollinger Bands, ATR\n",
        " Model Selection: Choose appropriate machine learning models such as time-series forecasting, regression,\n",
        "or deep learning approache\n",
        " Model Training: Train the selected model using the processed datase\n",
        " Model Evaluation: Assess model performance using metrics such as RMSE, MAE, and R¬≤ score\n",
        " Java + DSA\n",
        " Pwskills\n",
        "Model Optimization and Deploymen\n",
        " Hyperparameter Tuning: Optimize model parameters for better accurac\n",
        " Model Testing & Validation: Test the model on unseen data and analyze prediction\n",
        " Local Deployment: Deploy the trained model locally using Flask or Streamlit for testing\n",
        " Expected Deliverables\n",
        " 1. Machine Learning Mode\n",
        " A trained model that predicts cryptocurrency volatilit\n",
        " Evaluation metrics showing how well the model performs\n",
        " 2. Data Processing & Feature Engineerin\n",
        " Cleaned and prepared datase\n",
        " A brief explanation of new features added\n",
        " 3. Exploratory Data Analysis (EDA) Repor\n",
        " Summary of dataset statistic\n",
        " Basic visualizations (trends, correlations, distributions)\n",
        " 4. Project Documentatio\n",
        " High-Level Design (HLD) Document: Overview of system and architectur\n",
        " Low-Level Design (LLD) Document: Breakdown of how each component is implemente\n",
        " Pipeline Architecture: Explanation of data flow from preprocessing to predictio\n",
        " Final Report: A simple summary of findings, model performance, and key insights\n",
        " EDA Repor\n",
        " Guidelines & Submission Requirement\n",
        " Code Documentation: Ensure all scripts are well-commented and easy to follo\n",
        " Report Structure: The report must be structured and should clearly explain the methodology followe\n",
        " Diagrams & Visuals: Use appropriate diagrams and plots to explain datap rocessing, model selection, and\n",
        "performance evaluatio\n",
        " Deployment: If possible, deploy the model using a simple interface (e.g., Streamlit or Flask API) for testing\n",
        "predictions\n",
        " Submission Format\n",
        " The project must be submitted as a GitHub repository or a zipped folder containing\n",
        " Source Cod\n",
        " HLD & LLD Document\n",
        " Pipeline Architecture and Documentatio\n",
        " Final Report"
      ],
      "metadata": {
        "id": "vr0WEZAHL7qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Collection\n",
        "We'll first access the dataset from the provided link and inspect its structure.\n",
        "\n",
        "2. Data Preprocessing\n",
        "Handle missing values.\n",
        "\n",
        "Convert data types (e.g., datetime).\n",
        "\n",
        "Normalize numerical features.\n",
        "\n",
        "Encode categorical variables if needed.\n",
        "\n",
        "3. Feature Engineering\n",
        "We‚Äôll create:\n",
        "\n",
        "Rolling Volatility (e.g., standard deviation of returns).\n",
        "\n",
        "Moving averages (7-day, 30-day).\n",
        "\n",
        "Liquidity ratio: volume / market_cap.\n",
        "\n",
        "Technical indicators like Bollinger Bands, ATR.\n",
        "\n",
        "4. Exploratory Data Analysis (EDA)\n",
        "Visualize price and volume trends.\n",
        "\n",
        "Analyze volatility patterns.\n",
        "\n",
        "Correlation heatmaps.\n",
        "\n",
        "5. Model Selection & Training\n",
        "Options:\n",
        "\n",
        "XGBoost / Random Forest Regressor (for regression-based volatility prediction).\n",
        "\n",
        "LSTM / GRU (Deep Learning) for time-series forecasting.\n",
        "\n",
        "6. Model Evaluation\n",
        "Use:\n",
        "\n",
        "RMSE\n",
        "\n",
        "MAE\n",
        "\n",
        "R¬≤ Score\n",
        "\n",
        "7. Deployment (Optional)\n",
        "Local deployment with Streamlit.\n",
        "\n",
        "8. Deliverables\n",
        "We‚Äôll prepare:\n",
        "\n",
        "Code (well-commented)\n",
        "\n",
        "EDA report (plots + insights)\n",
        "\n",
        "HLD + LLD Docs\n",
        "\n",
        "Pipeline Architecture\n",
        "\n",
        "Final Report\n",
        "\n",
        "Thanks for uploading the dataset. Let's begin by:\n",
        "\n",
        "### üîç Step 1: Dataset Inspection\n",
        "\n",
        "\n",
        "* Load the dataset\n",
        "* Show you its first few rows\n",
        "* Check for missing values\n",
        "* Display basic statistics and column types\n",
        "\n",
        "\n",
        "### ‚úÖ Dataset Overview (72946 rows √ó 10 columns)\n",
        "\n",
        "| Column        | Type    | Description                        |\n",
        "| ------------- | ------- | ---------------------------------- |\n",
        "| `Unnamed: 0`  | int64   | Index-like column (can be dropped) |\n",
        "| `open`        | float64 | Opening price                      |\n",
        "| `high`        | float64 | Highest price of the day           |\n",
        "| `low`         | float64 | Lowest price of the day            |\n",
        "| `close`       | float64 | Closing price                      |\n",
        "| `volume`      | float64 | Trading volume                     |\n",
        "| `marketCap`   | float64 | Market capitalization              |\n",
        "| `timestamp`   | object  | Full timestamp                     |\n",
        "| `crypto_name` | object  | Name of cryptocurrency             |\n",
        "| `date`        | object  | Date (YYYY-MM-DD)                  |\n",
        "\n",
        "‚úÖ No missing values\n",
        "‚úÖ Clean numerical data\n",
        "üîÑ `volume` = 0 in many rows (will check later)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚è≠ Next Steps\n",
        "\n",
        "1. **Drop `Unnamed: 0`**\n",
        "\n",
        "2. **Convert `date` column to datetime**\n",
        "\n",
        "3. **Create new features**:\n",
        "\n",
        "   * Daily return\n",
        "   * Rolling volatility (e.g., 7-day, 30-day)\n",
        "   * Liquidity ratio: `volume / marketCap`\n",
        "   * Moving averages (7D, 30D)\n",
        "   * Technical indicators (e.g., Bollinger Bands)\n",
        "\n",
        "4. **Visualize Volatility (EDA)**\n",
        "\n",
        "5. **Train ML models to predict volatility**\n",
        "\n",
        "\n",
        "‚úÖ **Feature Engineering Completed**\n",
        "\n",
        "Here are the **new features** added:\n",
        "\n",
        "| Feature                               | Description                                                        |\n",
        "| ------------------------------------- | ------------------------------------------------------------------ |\n",
        "| `daily_return`                        | % change in closing price from the previous day                    |\n",
        "| `volatility_7d`                       | 7-day rolling standard deviation of daily returns                  |\n",
        "| `volatility_30d`                      | 30-day rolling standard deviation (smoother measure of volatility) |\n",
        "| `liquidity_ratio`                     | Volume / MarketCap, represents trading activity relative to size   |\n",
        "| `ma_7`, `ma_30`                       | 7-day and 30-day moving averages of close prices                   |\n",
        "| `bollinger_upper` / `bollinger_lower` | 20-day Bollinger Band levels (used in technical analysis)          |\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Next Step: Exploratory Data Analysis (EDA)\n",
        "\n",
        "\n",
        "\n",
        "* Plot price trends\n",
        "* Visualize volatility trends\n",
        "* Show correlation heatmap\n",
        "* Compare cryptocurrencies\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EeKhrx5PMNlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"your_dataset.csv\")  # replace with actual file path\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Drop unnecessary column if present\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# Sort and group\n",
        "df = df.sort_values(by=[\"crypto_name\", \"date\"])\n",
        "\n",
        "# Feature Engineering\n",
        "df['daily_return'] = df.groupby('crypto_name')['close'].pct_change()\n",
        "df['volatility_30d'] = df.groupby('crypto_name')['daily_return'].rolling(window=30).std().reset_index(0, drop=True)\n",
        "\n",
        "# Filter top cryptocurrencies\n",
        "top_cryptos = ['Bitcoin', 'Ethereum', 'Litecoin', 'XRP']\n",
        "df_top = df[df['crypto_name'].isin(top_cryptos)]\n",
        "\n",
        "# Plotting\n",
        "plt.style.use(\"ggplot\")\n",
        "fig, axes = plt.subplots(4, 2, figsize=(16, 20), sharex=True)\n",
        "\n",
        "for i, crypto in enumerate(top_cryptos):\n",
        "    crypto_data = df_top[df_top['crypto_name'] == crypto]\n",
        "\n",
        "    # Plot close price\n",
        "    axes[i][0].plot(crypto_data['date'], crypto_data['close'], label=f'{crypto} Price', color='blue')\n",
        "    axes[i][0].set_title(f'{crypto} - Closing Price')\n",
        "    axes[i][0].set_ylabel(\"Price (USD)\")\n",
        "    axes[i][0].legend()\n",
        "\n",
        "    # Plot volatility\n",
        "    axes[i][1].plot(crypto_data['date'], crypto_data['volatility_30d'], label=f'{crypto} Volatility (30d)', color='red')\n",
        "    axes[i][1].set_title(f'{crypto} - 30-Day Rolling Volatility')\n",
        "    axes[i][1].set_ylabel(\"Volatility\")\n",
        "    axes[i][1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pOOq5RLoMAfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Utilities ‚Äî src/utils.py Common helpers: metrics, saving/loading.\n",
        "# src/utils.py\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
        "\n",
        "def save_model(obj, path):\n",
        "    joblib.dump(obj, path)\n",
        "\n",
        "def load_model(path):\n",
        "    return joblib.load(path)\n",
        "\n",
        "def save_predictions(df, path):\n",
        "    df.to_csv(path, index=False)\n",
        "# 2) Data preprocessing & feature engineering ‚Äî src/data_preprocess.py .This loads raw CSV, creates features (daily_return, vol7, vol30, liquidity_ratio, ma_7, ma_30, bollinger bands), creates the target (next-day volatility_30d), drops NaNs and saves processed CSV.\n",
        "# src/data_preprocess.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "def preprocess(input_csv, output_csv):\n",
        "    df = pd.read_csv(input_csv)\n",
        "    # Drop index-like column if present\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "    # parse dates\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    # sort\n",
        "    df = df.sort_values(['crypto_name', 'date']).reset_index(drop=True)\n",
        "\n",
        "    # FEATURES\n",
        "    df['daily_return'] = df.groupby('crypto_name')['close'].pct_change()\n",
        "    df['volatility_7d'] = df.groupby('crypto_name')['daily_return'].rolling(window=7).std().reset_index(0, drop=True)\n",
        "    df['volatility_30d'] = df.groupby('crypto_name')['daily_return'].rolling(window=30).std().reset_index(0, drop=True)\n",
        "    df['liquidity_ratio'] = df['volume'] / (df['marketCap'] + 1e-10)\n",
        "    df['ma_7'] = df.groupby('crypto_name')['close'].transform(lambda x: x.rolling(window=7).mean())\n",
        "    df['ma_30'] = df.groupby('crypto_name')['close'].transform(lambda x: x.rolling(window=30).mean())\n",
        "    rolling_mean = df.groupby('crypto_name')['close'].transform(lambda x: x.rolling(window=20).mean())\n",
        "    rolling_std = df.groupby('crypto_name')['close'].transform(lambda x: x.rolling(window=20).std())\n",
        "    df['bollinger_upper'] = rolling_mean + 2 * rolling_std\n",
        "    df['bollinger_lower'] = rolling_mean - 2 * rolling_std\n",
        "\n",
        "    # Create a target: next-day volatility_30d (shift -1 per crypto). This predicts the next day's 30-day rolling vol.\n",
        "    df['target_vol30_next'] = df.groupby('crypto_name')['volatility_30d'].shift(-1)\n",
        "\n",
        "    # Drop rows with NaN in critical features or target\n",
        "    keep_cols = ['open','high','low','close','volume','marketCap','crypto_name','date',\n",
        "                 'daily_return','volatility_7d','volatility_30d','liquidity_ratio',\n",
        "                 'ma_7','ma_30','bollinger_upper','bollinger_lower','target_vol30_next']\n",
        "    df = df[keep_cols]\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    Path(output_csv).parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved processed data to {output_csv}. Rows: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", default=\"data/raw.csv\", help=\"raw csv path\")\n",
        "    parser.add_argument(\"--output\", default=\"data_processed/processed.csv\", help=\"processed output path\")\n",
        "    args = parser.parse_args()\n",
        "    preprocess(args.input, args.output)\n",
        "\n",
        "\n",
        "#3 EDA script ‚Äî src/eda.py Generates and saves visualization PNGs: price+volatility, correlation heatmap, target distribution.\n",
        "# src/eda.py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "def run_eda(processed_csv, out_dir=\"reports\"):\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "    df = pd.read_csv(processed_csv, parse_dates=['date'])\n",
        "\n",
        "    # Top 4 cryptos by rows\n",
        "    top = df['crypto_name'].value_counts().nlargest(4).index.tolist()\n",
        "    df_top = df[df['crypto_name'].isin(top)]\n",
        "\n",
        "    # Price and vol plots\n",
        "    for c in top:\n",
        "        tmp = df_top[df_top['crypto_name']==c]\n",
        "        fig, ax = plt.subplots(2,1, figsize=(12,8), sharex=True)\n",
        "        ax[0].plot(tmp['date'], tmp['close'])\n",
        "        ax[0].set_title(f'{c} - Close Price')\n",
        "        ax[0].set_ylabel('Price')\n",
        "        ax[1].plot(tmp['date'], tmp['volatility_30d'])\n",
        "        ax[1].set_title(f'{c} - 30-day Volatility')\n",
        "        ax[1].set_ylabel('Volatility')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{out_dir}/{c}_price_vol.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Correlation heatmap (numerics)\n",
        "    num_cols = ['open','high','low','close','volume','marketCap','daily_return','volatility_7d','volatility_30d','liquidity_ratio','ma_7','ma_30']\n",
        "    corr = df[num_cols].corr()\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
        "    plt.title(\"Feature Correlation\")\n",
        "    plt.savefig(f\"{out_dir}/correlation_heatmap.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Target distribution\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.histplot(df['target_vol30_next'], bins=100, kde=True)\n",
        "    plt.title(\"Distribution of target (next-day vol30)\")\n",
        "    plt.savefig(f\"{out_dir}/target_distribution.png\")\n",
        "    plt.close()\n",
        "    print(\"EDA plots saved to\", out_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", default=\"data_processed/processed.csv\")\n",
        "    parser.add_argument(\"--out\", default=\"reports\")\n",
        "    args = parser.parse_args()\n",
        "    run_eda(args.input, args.out)\n",
        "\n",
        "\n",
        "# 4) Train classical models (RandomForest / XGBoost) ‚Äî src/train_model.pyTrains a model pipeline (preprocessing + model), evaluates on time-based split (80/20 by date), saves model and transformer.\n",
        "# src/train_model.py\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import joblib\n",
        "from src.utils import regression_metrics, save_model, save_predictions\n",
        "\n",
        "def prepare_data(df):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    # split by date: training are rows where date <= quantile(0.8)\n",
        "    cutoff = df['date'].quantile(0.8)\n",
        "    train = df[df['date'] <= cutoff].copy()\n",
        "    test  = df[df['date'] > cutoff].copy()\n",
        "    return train, test\n",
        "\n",
        "def train(args):\n",
        "    Path(\"models\").mkdir(exist_ok=True)\n",
        "    df = pd.read_csv(args.input, parse_dates=['date'])\n",
        "    train, test = prepare_data(df)\n",
        "\n",
        "    features = ['open','high','low','close','volume','marketCap','daily_return',\n",
        "                'volatility_7d','volatility_30d','liquidity_ratio','ma_7','ma_30',\n",
        "                'bollinger_upper','bollinger_lower','crypto_name']\n",
        "    target = 'target_vol30_next'\n",
        "\n",
        "    X_train = train[features]\n",
        "    y_train = train[target]\n",
        "    X_test = test[features]\n",
        "    y_test = test[target]\n",
        "\n",
        "    numeric_features = [c for c in features if c != 'crypto_name']\n",
        "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), ['crypto_name'])\n",
        "    ])\n",
        "\n",
        "    if args.model == 'rf':\n",
        "        model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "        param_dist = {\n",
        "            \"model__n_estimators\": [100,200,400],\n",
        "            \"model__max_depth\": [5,10,20, None],\n",
        "            \"model__min_samples_split\": [2,5,10]\n",
        "        }\n",
        "    elif args.model == 'xgb':\n",
        "        model = xgb.XGBRegressor(n_jobs=-1, random_state=42, objective='reg:squarederror')\n",
        "        param_dist = {\n",
        "            \"model__n_estimators\": [100,200,400],\n",
        "            \"model__max_depth\": [3,6,10],\n",
        "            \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(\"model must be 'rf' or 'xgb'\")\n",
        "\n",
        "    pipe = Pipeline(steps=[('pre', preprocessor), ('model', model)])\n",
        "\n",
        "    # Randomized search with TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    search = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=10,\n",
        "                                cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=42, verbose=2)\n",
        "    print(\"Starting RandomizedSearchCV...\")\n",
        "    search.fit(X_train, y_train)\n",
        "    print(\"Best params:\", search.best_params_)\n",
        "\n",
        "    best = search.best_estimator_\n",
        "    # evaluate\n",
        "    y_pred = best.predict(X_test)\n",
        "    metrics = regression_metrics(y_test, y_pred)\n",
        "    print(\"Test metrics:\", metrics)\n",
        "\n",
        "    # Save model and also save predictions\n",
        "    save_model(best, f\"models/best_{args.model}.joblib\")\n",
        "    out_df = test[['date','crypto_name','close']].copy()\n",
        "    out_df['y_true'] = y_test.values\n",
        "    out_df['y_pred'] = y_pred\n",
        "    save_predictions(out_df, f\"models/predictions_{args.model}.csv\")\n",
        "    print(\"Saved model and predictions.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", default=\"data_processed/processed.csv\")\n",
        "    parser.add_argument(\"--model\", default=\"rf\", choices=['rf','xgb'])\n",
        "    args = parser.parse_args()\n",
        "    train(args)\n",
        "\n",
        "# 5) Evaluate / Plot results (inside training above we save predictions).\n",
        "You can load models/predictions_rf.csv and produce plots (time vs actual vs predicted). If you want a standalone evaluate.py, I can provide it ‚Äî let me know.\n",
        "\n",
        "6) Hyperparameter tuning (already integrated in train_model.py)\n",
        "RandomizedSearchCV was used with TimeSeriesSplit. If you want exhaustive GridSearchCV, swap to GridSearchCV but beware of runtime.\n",
        "\n",
        "7) LSTM for time-series (optional) ‚Äî src/train_lstm.py\n",
        "This shows a simple sequence-based LSTM across all cryptos (sliding windows).\n",
        "\n",
        "# src/train_lstm.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pathlib import Path\n",
        "\n",
        "def create_sequences(X, y, window=30):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - window):\n",
        "        Xs.append(X[i:(i+window)])\n",
        "        ys.append(y[i+window])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "def train_lstm(processed_csv, model_out=\"models/lstm.h5\", window=30, epochs=20, batch=64):\n",
        "    df = pd.read_csv(processed_csv, parse_dates=['date'])\n",
        "    # We'll build sequences per crypto and then concat\n",
        "    feature_cols = ['close','volume','marketCap','daily_return','volatility_7d','volatility_30d','liquidity_ratio','ma_7','ma_30']\n",
        "    X_all, y_all = [], []\n",
        "    for name, g in df.groupby('crypto_name'):\n",
        "        g = g.sort_values('date').reset_index(drop=True)\n",
        "        if len(g) < window + 2:\n",
        "            continue\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(g[feature_cols])\n",
        "        y = g['target_vol30_next'].values\n",
        "        Xs, ys = create_sequences(X_scaled, y, window=window)\n",
        "        X_all.append(Xs)\n",
        "        y_all.append(ys)\n",
        "    if not X_all:\n",
        "        raise RuntimeError(\"No crypto has enough rows for LSTM.\")\n",
        "    X = np.vstack(X_all)\n",
        "    y = np.hstack(y_all)\n",
        "    # shuffle, split train/test by index/time (we'll train/validate randomly here for simplicity)\n",
        "    idx = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:idx], X[idx:]\n",
        "    y_train, y_test = y[:idx], y[idx:]\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    Path(\"models\").mkdir(exist_ok=True)\n",
        "    es = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=epochs, batch_size=batch, callbacks=[es])\n",
        "    model.save(model_out)\n",
        "    print(\"Saved LSTM model to\", model_out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_lstm(\"data_processed/processed.csv\")\n",
        "\n",
        "#8.  Inference script ‚Äî src/predict.py . Load saved model (models/best_rf.joblib) and make predictions on a CSV input.\n",
        "\n",
        "# src/predict.py\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import joblib\n",
        "\n",
        "def predict(model_path, input_csv, output_csv):\n",
        "    model = joblib.load(model_path)\n",
        "    df = pd.read_csv(input_csv, parse_dates=['date'])\n",
        "    features = ['open','high','low','close','volume','marketCap','daily_return',\n",
        "                'volatility_7d','volatility_30d','liquidity_ratio','ma_7','ma_30',\n",
        "                'bollinger_upper','bollinger_lower','crypto_name']\n",
        "    X = df[features]\n",
        "    preds = model.predict(X)\n",
        "    df['pred_next_vol30'] = preds\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(\"Saved predictions to\", output_csv)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", default=\"models/best_rf.joblib\")\n",
        "    parser.add_argument(\"--input\", default=\"data_processed/processed.csv\")\n",
        "    parser.add_argument(\"--output\", default=\"models/predictions_inference.csv\")\n",
        "    args = parser.parse_args()\n",
        "    predict(args.model, args.input, args.output)\n",
        "\n",
        "#9. Streamlit simple app ‚Äî src/streamlit_app.py A tiny UI so anyone can upload a CSV and get predictions (assumes models/best_rf.joblib exists).\n",
        "\n",
        "# src/streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from io import StringIO\n",
        "\n",
        "st.title(\"Cryptocurrency Volatility Predictor\")\n",
        "\n",
        "model = joblib.load(\"models/best_rf.joblib\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload processed CSV (with features)\", type=[\"csv\"])\n",
        "if uploaded:\n",
        "    df = pd.read_csv(uploaded, parse_dates=['date'])\n",
        "    features = ['open','high','low','close','volume','marketCap','daily_return',\n",
        "                'volatility_7d','volatility_30d','liquidity_ratio','ma_7','ma_30',\n",
        "                'bollinger_upper','bollinger_lower','crypto_name']\n",
        "    X = df[features]\n",
        "    preds = model.predict(X)\n",
        "    df['pred_next_vol30'] = preds\n",
        "    st.write(\"Predictions (first 20 rows):\")\n",
        "    st.dataframe(df[['date','crypto_name','close','pred_next_vol30']].head(20))\n",
        "    csv = df.to_csv(index=False)\n",
        "    st.download_button(\"Download predictions CSV\", csv, \"predictions.csv\")\n",
        "else:\n",
        "    st.info(\"Upload the processed CSV (run preprocessing first).\")\n",
        "\n",
        "#10.  Simple Flask API ‚Äî src/flask_api.py. Small REST API that accepts JSON with rows and returns predictions.\n",
        "\n",
        "# src/flask_api.py\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "app = Flask(__name__)\n",
        "model = joblib.load(\"models/best_rf.joblib\")\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    data = request.get_json()\n",
        "    df = pd.DataFrame(data)\n",
        "    # expecting same feature columns as earlier\n",
        "    features = ['open','high','low','close','volume','marketCap','daily_return',\n",
        "                'volatility_7d','volatility_30d','liquidity_ratio','ma_7','ma_30',\n",
        "                'bollinger_upper','bollinger_lower','crypto_name']\n",
        "    X = df[features]\n",
        "    preds = model.predict(X)\n",
        "    return jsonify({\"predictions\": preds.tolist()})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000)\n",
        "\n",
        "#11. Documentation templates\n",
        "HLD.md (High Level Design)\n",
        "# High-Level Design (HLD)\n",
        "\n",
        "## Purpose\n",
        "Predict next-day 30-day rolling volatility for multiple cryptocurrencies.\n",
        "\n",
        "## Components\n",
        "- Data Ingestion: raw CSV (OHLC, volume, market cap)\n",
        "- Preprocessing: cleaning, datetime parse, feature engineering\n",
        "- Model Training: RandomForest/XGBoost or LSTM\n",
        "- Serving: Streamlit UI and Flask API\n",
        "- Monitoring: Save predictions/log metrics\n",
        "\n",
        "## Data Flow\n",
        "raw.csv -> data_preprocess.py -> processed.csv -> train_model.py -> models/* -> streamlit / flask (inference)\n",
        "\n",
        "# Low Level Design (LLD)\n",
        "\n",
        "## Preprocessing\n",
        "- Columns created:\n",
        "  - daily_return = pct_change(close)\n",
        "  - volatility_7d, volatility_30d = rolling std dev of daily_return\n",
        "  - liquidity_ratio = volume / marketCap\n",
        "  - ma_7, ma_30 = rolling means\n",
        "  - bollinger_upper/lower = 20-day mean ¬± 2*std\n",
        "- Target:\n",
        "  - target_vol30_next = volatility_30d.shift(-1) per crypto\n",
        "\n",
        "## Model Training\n",
        "- Pipeline:\n",
        "  - ColumnTransformer (StandardScaler for numeric, OneHotEncoder for crypto_name)\n",
        "  - Model: RandomForestRegressor or XGBRegressor\n",
        "- Time-based train/test split:\n",
        "  - cutoff = 80th percentile date across dataset\n",
        "\n",
        "## Deployment\n",
        "- Save trained Pipeline via joblib\n",
        "- Streamlit for UI, Flask for API\n",
        "\n",
        "\n",
        "#12. README.md (quick run steps)\n",
        "markdown\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "# Crypto Volatility Prediction\n",
        "\n",
        "1. Install dependencies:\n",
        "   `pip install -r requirements.txt`\n",
        "\n",
        "2. Place raw CSV as `data/raw.csv`\n",
        "\n",
        "3. Preprocess:\n",
        "   `python src/data_preprocess.py --input data/raw.csv --output data_processed/processed.csv`\n",
        "\n",
        "4. EDA:\n",
        "   `python src/eda.py --input data_processed/processed.csv --out reports`\n",
        "\n",
        "5. Train model (RandomForest example):\n",
        "   `python src/train_model.py --input data_processed/processed.csv --model rf`\n",
        "\n",
        "6. Predict:\n",
        "   `python src/predict.py --model models/best_rf.joblib --input data_processed/processed.csv --output models/predictions_inference.csv`\n",
        "\n",
        "7. Run Streamlit:\n",
        "   `streamlit run src/streamlit_app.py`\n",
        "\n",
        "8. Run Flask API:\n",
        "   `python src/flask_api.py`\n"
      ],
      "metadata": {
        "id": "HXTl_uPdNxBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}